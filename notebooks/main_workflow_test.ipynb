{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transformer Model Data Preparation Workflow",
   "id": "9b4d9a25d3fc9dac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook documents the initial pipeline for mass spectrometry data preparation and preprocessing, which is essential for feeding our future Transformer model.\n",
    "\n",
    "We start by loading and processing MGF files, applying a series of filters and transformations to ensure the quality and appropriate format of the data."
   ],
   "id": "6a69d1137df79e62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The first step involves validating the MGF file path (**path_check**) and loading the raw spectra. The **mgf_get_spectra** function is responsible for reading the MGF file and extracting each spectrum's data",
   "id": "cedc79057b837e53"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-30T17:13:30.009177Z",
     "start_time": "2025-06-30T17:13:28.488152Z"
    }
   },
   "source": [
    "from src.utils import *\n",
    "from src.config import *\n",
    "from src.mgf_tools.mgf_get import * "
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T17:13:30.016848Z",
     "start_time": "2025-06-30T17:13:30.012107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mgf_data = r\"/Users/carla/PycharmProjects/Mestrado/Transformer-Based-Models-for-Chemical-Fingerprint-Prediction/datasets/raw/cleaned_gnps_library.mgf\"\n",
    "\n",
    "path_check(mgf_data)"
   ],
   "id": "6dca8e4786c6f88c",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T17:18:40.089140Z",
     "start_time": "2025-06-30T17:13:33.779833Z"
    }
   },
   "cell_type": "code",
   "source": "mgf_spect= mgf_get_spectra(mgf_data, num_spectra=10)",
   "id": "a781ab36f38a82b7",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is the core phase of transforming the raw data. The **mgf_deconvoluter** function iterates over each loaded spectrum, applying a series of cleaning and tokenization steps via the **mgf_spectrum_deconvoluter**",
   "id": "2439e5b290345094"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T17:18:40.331694Z",
     "start_time": "2025-06-30T17:18:40.094834Z"
    }
   },
   "cell_type": "code",
   "source": "x = mgf_deconvoluter(mgf_data=mgf_spect, mz_vocabs=mz_vocabs, min_num_peaks=5, max_num_peaks=max_num_peaks, noise_rmv_threshold=0.01, mass_error=0.01, log=True)",
   "id": "aae912e960a915d8",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The **mgf_deconvoluter** function returns a list of tuples, where each tuple (spectrum_id, tokenized_mz, tokenized_precursor, intensities) represents a spectrum that has successfully passed through the entire preprocessing pipeline.",
   "id": "cfb65b4d21c7ecf5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T15:07:53.384628Z",
     "start_time": "2025-06-27T15:07:53.377589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if len(x) > 0:\n",
    "    spectrum_tuple = x[2]\n",
    "\n",
    "    spectrum_id, tokenized_mz, tokenized_precursor, intensities = spectrum_tuple\n",
    "\n",
    "    print(f\"\\nTokenised spectrum details:\")\n",
    "    print(f\"Spectrum ID: {spectrum_id}\")\n",
    "    print(f\"Number of m/z tokens: {len(tokenized_mz)}\")\n",
    "    print(f\"Number of intensities: {len(intensities)}\")\n",
    "    print(f\"Precursor token: {tokenized_precursor}\")\n",
    "\n",
    "else:\n",
    "    print(\"No spectrum passed through the filters and was processed\")"
   ],
   "id": "1d9062d6f986173c",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T17:18:46.759618Z",
     "start_time": "2025-06-30T17:18:40.335830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- IMPORTS NECESSÁRIOS PARA ESTA SECÇÃO ---\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np \n",
    "# Embora o collate_fn já lide com isso, é bom ter\n",
    "\n",
    "# Certifique-se de que as suas configurações estão importadas, especialmente max_seq_len e vocab_size\n",
    "from src.config import max_seq_len, vocab_size\n",
    "\n",
    "# Assumindo que a sua classe SpectraCollateFn está em src/data/collate_fn.py\n",
    "# (Se mudou o caminho da pasta 'model' para 'data', certifique-se que o import está correto)\n",
    "from src.data.collate_fn import SpectraCollateFn\n",
    "\n",
    "\n",
    "# --- 1. Definir a Classe Dataset (Simples e Reutilizável) ---\n",
    "# Esta classe irá encapsular a lista de tuplos processados pelo mgf_deconvoluter.\n",
    "class SpectrumDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for mass spectrometry data.\n",
    "    It wraps a list of pre-processed spectrum tuples (ID, tokenized_mz, tokenized_precursor, intensities)\n",
    "    and provides individual spectrum data for the DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, processed_spectra_list):\n",
    "        # processed_spectra_list é a lista de tuplos que o mgf_deconvoluter retorna\n",
    "        self.data = processed_spectra_list\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of processed spectra in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a single processed spectrum tuple by index.\n",
    "        This tuple is then passed to the collate_fn for batch processing.\n",
    "        \"\"\"\n",
    "        # Retorna o tuplo individual de um espectro:\n",
    "        # (spectrum_id, tokenized_mz_list, tokenized_precursor, intensity_array)\n",
    "        return self.data[idx]\n",
    "\n",
    "# --- 2. Preparar os Dados de Teste para o DataLoader ---\n",
    "# 'x' é a lista de tuplos gerada pelo mgf_deconvoluter na célula anterior\n",
    "# Ex: x = mgf_deconvoluter(...)\n",
    "\n",
    "if not x: # Verifica se a lista 'x' não está vazia (ou seja, se há espectros processados)\n",
    "    print(\"\\nAVISO: NENHUM espectro foi processado com sucesso pelo deconvoluter. O DataLoader estará vazio.\")\n",
    "    print(\"Verifique as suas configurações (`min_num_peaks`, `max_num_peaks`) e os ficheiros de input MGF.\")\n",
    "else:\n",
    "    print(f\"\\n--- Configurando o DataLoader ---\")\n",
    "    print(f\"Número total de espectros processados e prontos para o Dataset: {len(x)}\")\n",
    "\n",
    "    # --- 3. Instanciar o seu Dataset ---\n",
    "    # Passamos a lista 'x' (todos os espectros processados) para o Dataset\n",
    "    spectrum_dataset = SpectrumDataset(x)\n",
    "\n",
    "    # --- 4. Instanciar o seu Collate Function ---\n",
    "    # A sua SpectraCollateFn precisa do max_length (max_seq_len) e padding_token_value (vocab_size)\n",
    "    # Certifique-se que o construtor da sua SpectraCollateFn está de acordo com isto.\n",
    "    # Ex: class SpectraCollateFn: def __init__(self, max_length, padding_token_value): ...\n",
    "    # Se o seu __init__ já usa as variáveis globais, pode ser apenas my_collate_fn = SpectraCollateFn()\n",
    "    # Pelo código anterior, a sua SpectraCollateFn já lida com isto, então:\n",
    "    my_collate_fn = SpectraCollateFn() # Se o seu SpectraCollateFn usa os imports de config diretamente\n",
    "\n",
    "    # --- 5. Criar o DataLoader ---\n",
    "    batch_size = 2 # Um bom tamanho de batch para testar. Pode ajustar.\n",
    "    my_dataloader = DataLoader(\n",
    "        spectrum_dataset,    # O nosso Dataset\n",
    "        batch_size=batch_size, # O número de espectros por batch\n",
    "        shuffle=True,        # Shuffles os dados a cada época (bom para treino, opcional para teste)\n",
    "        collate_fn=my_collate_fn, # A nossa função de agrupamento personalizada\n",
    "        num_workers=0        # Para teste, 0 workers é mais simples. Aumentar para treino real.\n",
    "    )\n",
    "\n",
    "    print(f\"DataLoader configurado com batch_size={batch_size}.\")\n",
    "\n",
    "    # --- 6. Iterar sobre o DataLoader e inspecionar o primeiro Batch ---\n",
    "    print(f\"\\n--- Verificando o primeiro Batch do DataLoader ---\")\n",
    "    try:\n",
    "        # Pega o primeiro batch para inspeção\n",
    "        # next(iter(my_dataloader)) é uma forma comum de pegar um único batch\n",
    "        mz_batch, int_batch, mask_batch, ids_batch = next(iter(my_dataloader))\n",
    "\n",
    "        print(f\"\\nBatch obtido com sucesso!\")\n",
    "        print(f\"Shape de mz_batch (tokens de m/z e precursor, com padding): {mz_batch.shape}\")\n",
    "        print(f\"Shape de int_batch (intensidades e zeros, com padding): {int_batch.shape}\")\n",
    "        print(f\"Shape de mask_batch (máscara de atenção): {mask_batch.shape}\")\n",
    "        print(f\"IDs no primeiro batch: {ids_batch}\")\n",
    "\n",
    "        # Verificações de Sanidade (opcional, mas altamente recomendado)\n",
    "        # Assegura que o comprimento da sequência é MAX_SEQ_LEN\n",
    "        assert mz_batch.shape[1] == max_seq_len, f\"Comprimento da sequência incorreto! Esperado {max_seq_len}, obtido {mz_batch.shape[1]}\"\n",
    "        assert int_batch.shape[1] == max_seq_len, f\"Comprimento da sequência incorreto! Esperado {max_seq_len}, obtido {int_batch.shape[1]}\"\n",
    "        assert mask_batch.shape[1] == max_seq_len, f\"Comprimento da sequência incorreto! Esperado {max_seq_len}, obtido {mask_batch.shape[1]}\"\n",
    "\n",
    "        # Verificar se o dtype está correto\n",
    "        assert mz_batch.dtype == torch.long, f\"Dtype incorreto para mz_batch! Esperado torch.long, obtido {mz_batch.dtype}\"\n",
    "        assert int_batch.dtype == torch.float32, f\"Dtype incorreto para int_batch! Esperado torch.float32, obtido {int_batch.dtype}\"\n",
    "        assert mask_batch.dtype == torch.bool, f\"Dtype incorreto para mask_batch! Esperado torch.bool, obtido {mask_batch.dtype}\"\n",
    "\n",
    "        print(\"\\nVerificações de sanidade do shape e dtype concluídas com sucesso!\")\n",
    "\n",
    "        # Exemplo de um espectro dentro do batch (primeiro espectro do batch)\n",
    "        print(\"\\nPrimeiro espectro no batch (apenas os primeiros 15 e últimos 5 tokens/intensidades para ver o padding):\")\n",
    "        print(f\"  Tokens (mz_batch[0, :15]): {mz_batch[0, :15].tolist()}\")\n",
    "        print(f\"  Intensidades (int_batch[0, :15]): {int_batch[0, :15].tolist()}\")\n",
    "        print(f\"  Máscara (mask_batch[0, :15]): {mask_batch[0, :15].tolist()}\")\n",
    "\n",
    "        print(f\"  Tokens (mz_batch[0, -5:]): {mz_batch[0, -5:].tolist()}\")\n",
    "        print(f\"  Intensidades (int_batch[0, -5:]): {int_batch[0, -5:].tolist()}\")\n",
    "        print(f\"  Máscara (mask_batch[0, -5:]): {mask_batch[0, -5:].tolist()}\")\n",
    "\n",
    "        # Verificação do token de padding na máscara (se houver padding)\n",
    "        # Encontra o primeiro índice onde a máscara é False (significa padding)\n",
    "        first_padding_index = (mask_batch[0] == False).nonzero(as_tuple=True)\n",
    "        if first_padding_index[0].numel() > 0:\n",
    "            first_padding_index_val = first_padding_index[0][0].item()\n",
    "            print(f\"\\nPrimeiro índice de padding detectado no primeiro espectro do batch: {first_padding_index_val}\")\n",
    "            print(f\"Token esperado nesse índice: {vocab_size} (token de padding)\")\n",
    "            print(f\"Token real nesse índice: {mz_batch[0, first_padding_index_val].item()}\")\n",
    "            print(f\"Intensidade esperada nesse índice: 0.0\")\n",
    "            print(f\"Intensidade real nesse índice: {int_batch[0, first_padding_index_val].item()}\")\n",
    "        else:\n",
    "            print(\"\\nNão foi detetado padding no primeiro espectro do batch (o espectro preencheu o MAX_SEQ_LEN).\")\n",
    "\n",
    "\n",
    "    except StopIteration:\n",
    "        print(\"\\nErro: O DataLoader está vazio. Nenhum batch para processar. Isso pode acontecer se 'x' estiver vazio.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nOcorreu um erro ao processar o batch: {e}\")"
   ],
   "id": "ed186d9687458326",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T17:07:52.104889Z",
     "start_time": "2025-06-30T17:07:52.094631Z"
    }
   },
   "cell_type": "code",
   "source": "len(spectrum_dataset)",
   "id": "2676d18e449fee5b",
   "execution_count": 6,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
