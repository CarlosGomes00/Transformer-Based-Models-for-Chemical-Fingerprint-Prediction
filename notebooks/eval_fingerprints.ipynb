{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2a2d8a",
   "metadata": {},
   "source": [
    "# Real vs Pred fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287c106",
   "metadata": {},
   "source": [
    "This notebook is used to compare the fingerprints generated by the model with the actual fingerprints obtained through SMILES using Rdkit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd26447c",
   "metadata": {},
   "source": [
    "We start by importing the necessary packages and modules, as well as defining some important variables such as the seed and the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07396b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.Transformer import Transformer\n",
    "from src.utils import calculate_tanimoto\n",
    "from src.data.data_loader import *\n",
    "from src.data.mgf_tools.mgf_get import *\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "seed = ''\n",
    "mgf_path = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc77962d",
   "metadata": {},
   "source": [
    "This notebook is based on the assumption that a checkpoint of the trained model already exists, as well as its entire pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38558250",
   "metadata": {},
   "source": [
    "Therefore, let us look at some of the values related to the dataset and its respective processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533914af",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_dir = ''\n",
    "\n",
    "artifacts_dir = Path(artifacts_dir) / str(seed)\n",
    "\n",
    "with open(artifacts_dir / 'pipeline_config.json', 'r') as f:\n",
    "            pipeline_config = json.load(f)\n",
    "\n",
    "max_num_peaks = pipeline_config['max_num_peaks']\n",
    "max_seq_len = pipeline_config['max_seq_len']\n",
    "mz_vocabs = pipeline_config['mz_vocabs']\n",
    "vocab_size = pipeline_config['vocab_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a66cde",
   "metadata": {},
   "source": [
    "So, we created a data loader with the previous values. It is important to have the same seed as the split of the dataset to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = data_loader(seed=seed, batch_size=batch_size, num_workers=2, mgf_path=mgf_path, max_num_peaks=max_num_peaks, mz_vocabs=mz_vocabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546755d",
   "metadata": {},
   "source": [
    "Then, we choose the checkpoint of the model to use and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be6383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = ''\n",
    "\n",
    "model = Transformer.load_model(checkpoint_path, seed=seed)\n",
    "\n",
    "preds = model.predict(loaders['test'], return_probabilities=False, save_results=True)\n",
    "\n",
    "print(f'Preds size {len(preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02537db",
   "metadata": {},
   "source": [
    "Then, we use the data loader to retrieve the IDs of the predicted spectra, as well as their true fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18fc7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_list = []\n",
    "ids_list = []\n",
    "\n",
    "\n",
    "for batch in loaders['test']:\n",
    "    targes_batch = batch[4]\n",
    "    ids_batch = batch[3]\n",
    "\n",
    "    y_true_list.append(targes_batch.numpy())\n",
    "    ids_list.extend(ids_batch)\n",
    "\n",
    "\n",
    "y_true = np.vstack(y_true_list)\n",
    "\n",
    "print(f'True size {len(y_true)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77060426",
   "metadata": {},
   "source": [
    "Using the IDs, we go to the main dataset to retrieve the SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be97875",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = mgf_get_spectra(mgf_path)\n",
    "\n",
    "smiles_dict = {\n",
    "    espectro[\"params\"][\"spectrum_id\"]: espectro[\"params\"][\"smiles\"] \n",
    "    for espectro in full_dataset\n",
    "    if \"spectrum_id\" in espectro[\"params\"] and \"smiles\" in espectro[\"params\"]\n",
    "}\n",
    "\n",
    "smiles_list = [smiles_dict[spectrum_id] for spectrum_id in ids_list]\n",
    "\n",
    "print(f'Smiles size {len(smiles_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4bda72",
   "metadata": {},
   "source": [
    "And we confirmed whether everything went according to plan by checking the size of the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99466a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(preds) == len(y_true) == len(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f4258c",
   "metadata": {},
   "source": [
    "Next, we calculated the Tanimoto similarity between the predicted and real fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbfb76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tanimoto_scores = calculate_tanimoto(y_pred=preds, y_true=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49605de7",
   "metadata": {},
   "source": [
    "And we identified which ones had the best scores, and which ones had the worst scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d43c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = np.argsort(tanimoto_scores)\n",
    "\n",
    "worsts_idx = sorted_idx[:10]\n",
    "bests_idx = sorted_idx[-10:][::-1]\n",
    "\n",
    "analysis_data = []\n",
    "\n",
    "print('Top 10 worsts scores')\n",
    "for idx in worsts_idx:\n",
    "    score = tanimoto_scores[idx]\n",
    "    spectrum_id = ids_list[idx]\n",
    "    smiles = smiles_list[idx]\n",
    "\n",
    "    analysis_data.append({\n",
    "        'Category': 'Worst',\n",
    "        'Spectrum_ID': spectrum_id,\n",
    "        'Tanimoto_Score' : score,\n",
    "        'SMILES' : smiles\n",
    "    })\n",
    "\n",
    "\n",
    "print('Top 10 bests scores')\n",
    "for idx in bests_idx:\n",
    "    score = tanimoto_scores[idx]\n",
    "    spectrum_id = ids_list[idx]\n",
    "    smiles = smiles_list[idx]\n",
    "\n",
    "    \n",
    "    analysis_data.append({\n",
    "        'Category': 'Best',\n",
    "        'Spectrum_ID': spectrum_id,\n",
    "        'Tanimoto_Score' : score,\n",
    "        'SMILES' : smiles\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215598bf",
   "metadata": {},
   "source": [
    "And we save everything in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.DataFrame(analysis_data)\n",
    "\n",
    "df_analysis.to_csv('error_analysis_top_bottom_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d269405",
   "metadata": {},
   "source": [
    "Then, we can use rdkit to understand the structure of the compounds with the best and worst scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0493cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def desenhar_grelha_rdkit(df, categoria):\n",
    "\n",
    "    df_filtrado = df[df['Category'] == categoria]\n",
    "\n",
    "    mols = []\n",
    "    legendas = []\n",
    "\n",
    "    for _, row in df_filtrado.iterrows():\n",
    "        smile = row['SMILES']\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        \n",
    "        if mol is not None:\n",
    "            mols.append(mol)\n",
    "            legenda = f\"#{row['Spectrum_ID']}\\nTanimoto: {row['Tanimoto_Score']:.2f}\"\n",
    "            legendas.append(legenda)\n",
    "        else:\n",
    "            print(f\"RDKit couldn't interpret the SMILES.: {smile}\")\n",
    "    \n",
    "    img = Draw.MolsToGridImage(\n",
    "        mols, \n",
    "        molsPerRow=5, \n",
    "        subImgSize=(350, 300), \n",
    "        legends=legendas,\n",
    "        returnPNG=False \n",
    "    )\n",
    "\n",
    "    display(img)\n",
    "\n",
    "    img.save(f'image_{categoria}_predictions.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5306173",
   "metadata": {},
   "outputs": [],
   "source": [
    "desenhar_grelha_rdkit(df_analysis, 'Best')\n",
    "desenhar_grelha_rdkit(df_analysis, 'Worst')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
